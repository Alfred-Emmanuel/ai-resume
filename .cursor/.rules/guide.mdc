---
alwaysApply: true
---

# Roadmap — step-by-step (phases & tasks)

---

# Phase 0 — Prep & scaffolding (project foundation)

**Goal:** create repo, infra accounts, and dev environment so you can iterate quickly and safely.

**Tasks**

1. Create monorepo (pnpm workspaces / Turborepo) skeleton:

   * `apps/backend`, `apps/website`, `apps/extension`, `packages/types`, `packages/utils`.
   * Add `pnpm-workspace.yaml`, `tsconfig.base.json`, root `package.json`.
2. Initialize GitHub repo + branch policy.
3. Create cloud accounts & resources:

   * S3 bucket (or MinIO for local), PostgreSQL instance, Redis (or local Redis), Docker Hub (or ECR), domain, email provider.
4. Register external services:

   * OpenAI (or chosen LLM vendor) API key, Stripe + Paystack accounts, Chrome Web Store developer account.
5. Create `.env.example` and secret management plan (use Secrets Manager).
6. Add CI template (GitHub Actions) stub that runs lint/test/build jobs for apps.
7. Create a local dev `docker-compose.yml` for Postgres, Redis, MinIO.

**Acceptance**

* `pnpm install` succeeds at repo root.
* Dev containers start (Postgres, Redis, MinIO).
* CI runs empty tests on PR.

---

# Phase 1 — Core backend & resume upload + parsing (foundation of data)

**Goal:** let users upload a resume and produce a canonical JSON representation.

**Tasks**

1. Backend skeleton (Express/NestJS) with auth and basic routing.

   * Implement `POST /api/v1/auth/signup`, `POST /api/v1/auth/login` (JWT).
2. File upload endpoint `POST /api/v1/resumes/upload`:

   * Accept PDF/DOCX, store raw file in S3/MinIO.
   * Save a resume record in DB (file key, user\_id).
3. Text extraction and parsing pipeline:

   * Integrate PDF & DOCX extractors (e.g., `pdfplumber` / `mammoth` or `pdf-parse` + `docx`).
   * Run heuristic parser that extracts sections (Contact, Summary, Experience\[], Skills\[], Education\[]).
   * Save canonical JSON to DB `resumes.canonical_json`.
   * Unit tests for parser on sample resumes (include 10 varied sample resumes).
4. Website: simple UI to sign up, upload resume, and show parsed canonical JSON for review.

   * Endpoint `GET /api/v1/resumes/:id` to fetch parsed JSON.
5. Small admin/debug UI to correct the parsed JSON (manual edits).

**Acceptance**

* Upload returns `resume_id`, file saved in S3, `canonical_json` stored.
* UI shows parsed JSON and allows manual corrections.
* Parser unit tests pass.

---

# Phase 2 — LLM pipeline (text-only generation + validation)

**Goal:** wire up the LLM (initially as a stub) to produce a tailored resume text and cover letter from canonical JSON + job text.

**Tasks**

1. Create LLM wrapper service with 2 modes:

   * **Stub mode** for local dev that returns deterministic rewrites.
   * **Prod mode** to call OpenAI/Anthropic with prompt templates.
2. Implement `POST /api/v1/jobs/capture` (job text ingestion) — accepts job title, description, URL and stores normalized job record.
3. Implement `POST /api/v1/generate/resume` (text-only run):

   * Input: `user_id`, `resume_id`, `job_id`, options (`preview_only: true`).
   * Backend loads canonical JSON + job normalized text → calls LLM service with the resume-tailor prompt.
   * Run hallucination guard: validate generated content against canonical JSON (no new employers/dates).
   * Return `generated_text` and `diff` (what changed) to client.
4. Build `POST /api/v1/generate/coverletter` for cover letter text.
5. Website popup + extension UI: add “Preview generated text” flow; allow user to approve/reject changes.
6. Unit/integration tests for prompt output + hallucination guard.

**Acceptance**

* `generate/resume` returns tailored text that contains no invented employers/dates; guard flags and rejects bad outputs.
* UI shows preview and diffs; user can accept or request rework.

---

# Phase 3 — File export & worker pipeline (DOCX / PDF production)

**Goal:** convert approved text into downloadable DOCX/PDF and produce a presigned download URL.

**Tasks**

1. Implement worker queue (Redis + BullMQ) for heavy tasks (rendering PDF/DOCX).
2. Implement renderer service:

   * Convert structured output -> HTML templates (ATS-safe CSS).
   * Use headless Chromium (Puppeteer) to render HTML → PDF.
   * Use `docx`/`html-docx-js` to produce DOCX.
3. New API `POST /api/v1/generate/finish`:

   * Accepts approved generated text and queues a render job.
   * Worker generates files, uploads to S3, stores `generation` record with `generated_file_key`.
4. `GET /api/v1/generate/:id/download` returns presigned URL.
5. Extension & website trigger and receive presigned URL -> initiate download.
6. Monitor for rendering errors and implement retry/backoff.

**Acceptance**

* Given approved generated text, user can download valid PDF and DOCX files.
* File correctness: no images, header/footers do not hide text, and plain textual flow (ATS-friendly).
* Worker retry/rescue logic handles failures.

---

# Phase 4 — Browser extension MVP (client side integration)

**Goal:** build the extension that captures jobs, calls the backend, and downloads generated resumes.

**Tasks**

1. Extension content script:

   * Implement DOM selectors for LinkedIn / Indeed to extract job title, company, location, and description.
   * Provide a fallback: allow user to paste/copy job text.
2. Background script + popup UI:

   * Handle OAuth flow / JWT retrieval by opening website auth flow.
   * Popup shows parsed job info + buttons: `Generate Preview`, `Generate File`, `Generate Cover Letter`.
3. Integrate callflows:

   * Popup calls `POST /jobs/capture` to create job.
   * Calls `POST /generate/resume` to get preview.
   * On approval, calls `POST /generate/finish` -> polls `GET /generate/:id/status` -> obtains presigned URL.
4. Implement download & show to user.
5. Add UX: show which ATS fixes are applied, and let user accept/reject each bullet (if Phase 2 created that UI).
6. Pack extension and test locally (load unpacked), then prepare submission for Chrome Web Store.

**Acceptance**

* Extension successfully extracts job on LinkedIn/Indeed and produces downloadable resume files.
* Auth flow works without exposing credentials.
* Downloads begin from presigned URL without proxying files through your servers.

---

# Phase 5 — Authentication, billing & quota enforcement

**Goal:** protect paid features, implement subscription plans, and enforce generation quotas.

**Tasks**

1. Integrate Stripe and Paystack:

   * Create products/plans and store `subscription` records.
   * Implement `POST /api/v1/webhook/stripe` to update subscription status.
2. Build usage & quota system:

   * Log `generation_requested` and `generation_completed` events.
   * Enforce quotas in `POST /api/v1/generate/resume` (reject or queue if exceeds).
3. Add billing UI in website (manage plan, card, invoices).
4. Add admin UI for refunds, plan changes.
5. Tests for webhook handling and quota enforcement.

**Acceptance**

* Subscription purchase changes `subscription` DB and unlocks features.
* Over-quota generation requests are blocked and return a clear error.

---

# Phase 6 — Quality of life & product polish (UX & acceptance flow)

**Goal:** add user controls, review UI, improve generation accuracy, and make product beta-ready.

**Tasks**

1. Build in-line reviewer UI where the user can:

   * See changes per bullet and accept/reject edits before finalizing.
   * Manually add/modify bullets to keep truthful.
2. Add “why this change” tooltips showing which keywords were used and why.
3. Add history dashboard with downloads, job metadata, generation prompts used.
4. Expand job board support (Glassdoor, company pages).
5. Add tests for common resume edge cases (two-column PDFs, images).
6. Implement rate-limiting & abuse detection in backend.

**Acceptance**

* Users can fine-tune generated resumes via UI before download.
* Key success metrics: preview accept rate (internal), errors rate < threshold.

---

# Phase 7 — Scale, embeddings & semantic matching

**Goal:** move from token-overlap heuristics to semantic matching for better keyword alignment and reuse.

**Tasks**

1. Add embeddings pipeline:

   * Compute embeddings for experience bullets and store in Vector DB (Pinecone/Redis/Weaviate).
   * Compute job embeddings to retrieve best matching bullets for the job description.
2. Use hybrid ranking (embedding similarity + keyword match) to choose bullets & craft phrasing.
3. Cache frequent job patterns and generation results to reduce calls/costs.
4. Monitor LLM cost per generation and add fallbacks (smaller models / fewer tokens) when appropriate.

**Acceptance**

* New matching improves internal relevance metrics (higher similarity scores).
* Token usage per generation reduced via caching & model selection logic.

---

# Phase 8 — Security, compliance, and legal

**Goal:** make the product compliant with privacy laws and safe to operate.

**Tasks**

1. Add data deletion endpoint `DELETE /api/v1/user/:id/data` and implement safe purge from S3, DB, vector DB.
2. Encrypt PII at rest and in transit; rotate secrets periodically.
3. Add Terms of Service, Privacy Policy, and Acceptable Use Policy to website.
4. Perform security audit, dependency scan, and penetration testing.
5. Add logging and audits for admin actions and access.

**Acceptance**

* GDPR right-to-be-forgotten implemented and tested.
* Keys rotated and no plaintext secrets in repo.

---

# Phase 9 — Release & grow (publishing + marketing)

**Goal:** publish extension, open beta, iterate on growth channels.

**Tasks**

1. Package and submit extension to Chrome Web Store and Firefox Add-ons.
2. Run closed beta invite flow: onboard initial users, collect qualitative feedback, run surveys for resume->interview lift.
3. Launch marketing: SEO pages (“beat the ATS”), blog posts, partnerships with career coaches/universities.
4. Monitor telemetry, iterate prioritizing feature requests and error fixes.

**Acceptance**

* Extension published and available.
* Beta cohort engaged with measurable usage and feedback.

---

# Integration — How everything actually connects (end-to-end flow)

This is the single canonical sequence the product runs on each user job:

1. User installs extension and authenticates (JWT obtained via website OAuth).
2. On a job page, the extension content script extracts job title, company, location, and job description (client-side).
3. Extension popup calls `POST /api/v1/jobs/capture` with job text and `user_id`.
4. User chooses “Generate Resume” in popup:

   * Popup sends `POST /api/v1/generate/resume` with `user_id`, `resume_id`, `job_id`, and options (`preview_only: true`).
5. Backend loads `resumes.canonical_json` from DB (or S3).
6. Backend builds prompt (canonical JSON + job normalized fields) and calls LLM service (or stub).
7. LLM returns `generated_text` (structured JSON), backend runs hallucination guard to ensure no invented facts.
8. Backend returns preview to extension/website. User reviews & approves.
9. User approves → front-end calls `POST /api/v1/generate/finish`.
10. Backend enqueues a render job (BullMQ) with the approved structured output.
11. Worker renders HTML → PDF/DOCX, uploads to S3, updates `generations` row with `generated_file_key`.
12. Backend notifies client; client calls `GET /api/v1/generate/:id/download` and receives presigned URL.
13. Extension triggers browser download; backend logs event and decrements quota.
14. Billing webhooks reconcile usage and billing provider events.

**Example payloads**

* `POST /api/v1/generate/resume`:

```json
{
  "user_id":"user-uuid",
  "resume_id":"resume-uuid",
  "job_id":"job-uuid",
  "options": { "format":"pdf", "preview_only": true }
}
```

* `Generation` record:

```json
{ "id":"gen-uuid", "user_id":"user-uuid", "resume_id":"...", "job_id":"...", "status":"queued" }
```

---

# Acceptance criteria checklist (project-level)

* Users can upload a resume and review parsed canonical JSON.
* Extension extracts job postings from LinkedIn/Indeed and initiates generation.
* Backend produces an ATS-friendly PDF and DOCX with no invented employers/dates.
* Billing enforces quotas and accepts payments (Stripe/Paystack).
* Users can download files directly via presigned URLs.
* Data deletion and GDPR requests are honored.
* Basic metrics & logs exist for monitoring (generation counts, latencies, cost).

---

# Developer tooling & local dev commands (practical)

* Run local infra: `docker-compose up postgres redis minio`
* Start backend: `pnpm --filter backend dev`
* Start website: `pnpm --filter website dev`
* Start extension dev: `pnpm --filter extension dev` (or use `web-ext run --source ./apps/extension/dist`)
* Run workers: `pnpm --filter backend start-worker` (worker process)
* Run tests: `pnpm test` or `pnpm --filter backend test`

---

<!-- # Next recommended actions (what to do right now)

1. Initialize monorepo scaffold (I can generate starter files for you now).
2. Implement Phase 1 (upload + parser) — this will let you iterate quickly on user flows.
3. Add LLM stub and implement Phase 2 (text-only generation + preview).
4. Build Phase 4 extension MVP in parallel with Phase 2 so you can test end-to-end early.

---

If you want, I’ll immediately produce one of these next artifacts (pick one) and generate it for you now:

* A) Monorepo bootstrap files (`pnpm-workspace.yaml`, `package.json`, `tsconfig.base.json`) ready to copy.
* B) Starter backend generation endpoint + parser skeleton (TypeScript + Express + sample tests).
* C) Starter extension content script + popup skeleton (React + Vite + manifest v3).
* D) A GitHub issues list (one issue per task above) you can paste into GitHub to track progress.

Which would you like me to generate right now? -->
